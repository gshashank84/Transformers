{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c83a079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12d8f490",
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention(keras.layers.Layer):\n",
    "    def __init__(self, units=512, dk=64, dv=64):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        self.dk = dk\n",
    "        self.dv = dv\n",
    "        self.scale_factor = 1/(self.dk**(1/2))\n",
    "        self.softmax = tf.keras.layers.Softmax(axis=-1)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        #W_query\n",
    "        self.w_q = self.add_weight(shape=(self.units, self.dk),\n",
    "                                   initializer=\"random_normal\", \n",
    "                                   trainable=True)\n",
    "        #W_key\n",
    "        self.w_k = self.add_weight(shape=(self.units, self.dk),\n",
    "                                   initializer=\"random_normal\", \n",
    "                                   trainable=True)\n",
    "        #W_value\n",
    "        self.w_v = self.add_weight(shape=(self.units, self.dv),\n",
    "                                   initializer=\"random_normal\", \n",
    "                                   trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query = tf.matmul(inputs, self.w_q)\n",
    "        key = tf.matmul(inputs, self.w_k)\n",
    "        value = tf.matmul(inputs, self.w_v)\n",
    "        \n",
    "        # Q x K_T\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        # Summation: [q1k1+q1k2+q1k3+.., q2k1+q2k2+q2k3+.., ..]\n",
    "        score = tf.reduce_sum(score, axis=2, keepdims=True)\n",
    "        # Scaling the score with 1/sqrt(dk)\n",
    "        score = score*self.scale_factor\n",
    "        # Using softmax\n",
    "        normalized_score = self.softmax(score)\n",
    "        \n",
    "        z = normalized_score*value\n",
    "        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a33a388",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_head_attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_head=8, units=512):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.units = units # <- d_model\n",
    "        self.dk = self.units//self.n_head\n",
    "        self.dv = self.dk\n",
    "        self.attention = self_attention(units=self.units, dk=self.dk, dv=self.dv)\n",
    "        self.concat = tf.keras.layers.Concatenate(axis=-1)\n",
    "        self.W = self.add_weight(shape=(self.n_head*self.dv, self.units),\n",
    "                                   initializer=\"random_normal\", \n",
    "                                   trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        head_list = []\n",
    "        for _ in range(self.n_head):\n",
    "            head = self.attention(inputs)\n",
    "            head_list.append(head)\n",
    "            \n",
    "        concat_heads = self.concat(head_list)\n",
    "        z = tf.matmul(concat_heads, self.W)\n",
    "        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc9786c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class add_normalize(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.normalize = tf.keras.layers.BatchNormalization()#tf.keras.layers.LayerNormalization()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Addition\n",
    "        x1 = self.add([inputs[0], inputs[1]])\n",
    "        # Normalization\n",
    "        x2 = self.normalize(x1)\n",
    "        \n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d169f26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, units=512, n_head=8):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        self.n_head = n_head\n",
    "        self.self_attention = multi_head_attention(self.n_head, self.units)\n",
    "        self.add_normalize = add_normalize()\n",
    "        self.dense = tf.keras.layers.Dense(self.units, \n",
    "                                           activation='relu',\n",
    "                                           kernel_initializer='glorot_uniform')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Self-Attention\n",
    "        x1 = self.self_attention(inputs)\n",
    "        \n",
    "        # Add & Normalize\n",
    "        x2 = self.add_normalize([inputs, x1])\n",
    "        \n",
    "        # Feed Forward\n",
    "        x3 = self.dense(x2)\n",
    "        \n",
    "        # Add & Normalize\n",
    "        x4 = self.add_normalize([x2, x3])\n",
    "        \n",
    "        return x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ba64118",
   "metadata": {},
   "outputs": [],
   "source": [
    "class bert_base(tf.keras.layers.Layer):\n",
    "    def __init__(self, L=12, H=768, A=12):\n",
    "        super().__init__()\n",
    "        # L -> Number of transformer blocks (encoders)\n",
    "        # H -> Hidden vector size\n",
    "        # A -> Number of Attention Heads\n",
    "        self.L = L\n",
    "        self.H = H\n",
    "        self.A = A\n",
    "        \n",
    "        # transformer block (encoder)\n",
    "        self.encoder_block = encoder(units=self.H, n_head=self.A)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for _ in range(self.L):\n",
    "            x = self.encoder_block(x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9c694fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(10,768,))\n",
    "outputs = bert_base(L=12, H=768, A=12)(inputs)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2985e0ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 10, 768)]         0         \n",
      "_________________________________________________________________\n",
      "bert_base (bert_base)        (None, 10, 768)           1330944   \n",
      "=================================================================\n",
      "Total params: 1,330,944\n",
      "Trainable params: 1,329,408\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0a90e44",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.utils.plot_model(model, \"Bert_Base.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2221abe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
